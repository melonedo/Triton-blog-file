--- matmul/with_dot_bf16/MLIR/35-TritonGPUPipeline.mlir
+++ matmul/with_dot_bf16/MLIR/36-Canonicalizer.mlir
@@ -98,48 +98,46 @@
     %77 = arith.andi %76, %74 : tensor<64x64xi1, #blocked> loc(#loc25)
     %78 = ttg.async_copy_global_to_local %73, %75 mask %77 other %cst_0 : tensor<64x64x!tt.ptr<bf16>, #blocked> -> <64x64xbf16, #shared, #smem, mutable, 2x64x64> loc(#loc24)
     %79 = ttg.async_commit_group %78 loc(#loc24)
-    %80:9 = scf.for %arg9 = %c0_i32 to %38 step %c1_i32 iter_args(%arg10 = %cst_2, %arg11 = %c1_i32, %arg12 = %c-1_i32, %arg13 = %c2_i32, %arg14 = %c2_i32, %arg15 = %49, %arg16 = %68, %arg17 = %57, %arg18 = %79) -> (tensor<128x64xf32, #mma>, i32, i32, i32, i32, !ttg.async.token, !ttg.async.token, !ttg.async.token, !ttg.async.token)  : i32 {
+    %80:7 = scf.for %arg9 = %c0_i32 to %38 step %c1_i32 iter_args(%arg10 = %cst_2, %arg11 = %c1_i32, %arg12 = %c-1_i32, %arg13 = %49, %arg14 = %68, %arg15 = %57, %arg16 = %79) -> (tensor<128x64xf32, #mma>, i32, i32, !ttg.async.token, !ttg.async.token, !ttg.async.token, !ttg.async.token)  : i32 {
       %97 = arith.subi %38, %c2_i32 : i32 loc(#loc25)
       %98 = arith.cmpi slt, %arg9, %97 : i32 loc(#loc25)
       %99 = arith.addi %arg12, %c1_i32 : i32 loc(#loc25)
-      %100 = arith.cmpi sge, %99, %arg13 : i32 loc(#loc25)
+      %100 = arith.cmpi sge, %99, %c2_i32 : i32 loc(#loc25)
       %101 = arith.select %100, %c0_i32, %99 : i32 loc(#loc25)
-      %102 = ttg.async_wait %arg15, %arg17 {num = 2 : i32} loc(#loc23)
+      %102 = ttg.async_wait %arg13, %arg15 {num = 2 : i32} loc(#loc23)
       %103 = ttg.memdesc_subview %39[%101, %c0_i32, %c0_i32] : !ttg.memdesc<2x128x64xbf16, #shared, #smem, mutable> -> !ttg.memdesc<128x64xbf16, #shared, #smem, mutable, 2x128x64> loc(#loc23)
-      %104 = ttg.local_load %103 token %102 : !ttg.memdesc<128x64xbf16, #shared, #smem, mutable, 2x128x64> -> tensor<128x64xbf16, #blocked> loc(#loc23)
+      %104 = ttg.local_load %103 token %102 : !ttg.memdesc<128x64xbf16, #shared, #smem, mutable, 2x128x64> -> tensor<128x64xbf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 2}>> loc(#loc23)
       %105 = ttg.memdesc_subview %40[%101, %c0_i32, %c0_i32] : !ttg.memdesc<2x64x64xbf16, #shared, #smem, mutable> -> !ttg.memdesc<64x64xbf16, #shared, #smem, mutable, 2x64x64> loc(#loc24)
-      %106 = ttg.local_load %105 token %102 : !ttg.memdesc<64x64xbf16, #shared, #smem, mutable, 2x64x64> -> tensor<64x64xbf16, #blocked> loc(#loc24)
-      %107 = ttg.convert_layout %104 : tensor<128x64xbf16, #blocked> -> tensor<128x64xbf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 2}>> loc(#loc23)
-      %108 = ttg.convert_layout %106 : tensor<64x64xbf16, #blocked> -> tensor<64x64xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 2}>> loc(#loc24)
-      %109 = tt.dot %107, %108, %arg10, inputPrecision = tf32 : tensor<128x64xbf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 2}>> * tensor<64x64xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 2}>> -> tensor<128x64xf32, #mma> loc(#loc32)
-      %110 = arith.addi %arg11, %c1_i32 : i32 loc(#loc25)
-      %111 = arith.cmpi sge, %110, %c2_i32 : i32 loc(#loc25)
-      %112 = arith.select %111, %c0_i32, %110 : i32 loc(#loc25)
-      %113 = arith.addi %arg9, %c2_i32 : i32 loc(#loc25)
-      %114 = arith.muli %113, %c64_i32 : i32 loc(#loc33)
-      %115 = arith.subi %arg5, %114 : i32 loc(#loc28)
-      %116 = tt.splat %115 : i32 -> tensor<1x64xi32, #blocked> loc(#loc26)
-      %117 = arith.cmpi slt, %22, %116 : tensor<1x64xi32, #blocked> loc(#loc26)
-      %118 = tt.splat %114 : i32 -> tensor<128x64xi32, #blocked> loc(#loc29)
-      %119 = tt.addptr %25, %118 : tensor<128x64x!tt.ptr<bf16>, #blocked>, tensor<128x64xi32, #blocked> loc(#loc29)
-      %120 = tt.broadcast %117 : tensor<1x64xi1, #blocked> -> tensor<128x64xi1, #blocked> loc(#loc23)
-      %121 = ttg.memdesc_subview %39[%112, %c0_i32, %c0_i32] : !ttg.memdesc<2x128x64xbf16, #shared, #smem, mutable> -> !ttg.memdesc<128x64xbf16, #shared, #smem, mutable, 2x128x64> loc(#loc23)
-      %122 = tt.splat %98 : i1 -> tensor<128x64xi1, #blocked> loc(#loc25)
-      %123 = arith.andi %122, %120 : tensor<128x64xi1, #blocked> loc(#loc25)
-      %124 = ttg.async_copy_global_to_local %119, %121 mask %123 other %cst_1 : tensor<128x64x!tt.ptr<bf16>, #blocked> -> <128x64xbf16, #shared, #smem, mutable, 2x128x64> loc(#loc23)
-      %125 = ttg.async_commit_group %124 loc(#loc23)
-      %126 = tt.splat %115 : i32 -> tensor<64x1xi32, #blocked> loc(#loc27)
-      %127 = arith.cmpi slt, %27, %126 : tensor<64x1xi32, #blocked> loc(#loc27)
-      %128 = arith.muli %114, %arg7 : i32 loc(#loc30)
-      %129 = tt.splat %128 : i32 -> tensor<64x64xi32, #blocked> loc(#loc31)
-      %130 = tt.addptr %36, %129 : tensor<64x64x!tt.ptr<bf16>, #blocked>, tensor<64x64xi32, #blocked> loc(#loc31)
-      %131 = tt.broadcast %127 : tensor<64x1xi1, #blocked> -> tensor<64x64xi1, #blocked> loc(#loc24)
-      %132 = ttg.memdesc_subview %40[%112, %c0_i32, %c0_i32] : !ttg.memdesc<2x64x64xbf16, #shared, #smem, mutable> -> !ttg.memdesc<64x64xbf16, #shared, #smem, mutable, 2x64x64> loc(#loc24)
-      %133 = tt.splat %98 : i1 -> tensor<64x64xi1, #blocked> loc(#loc25)
-      %134 = arith.andi %133, %131 : tensor<64x64xi1, #blocked> loc(#loc25)
-      %135 = ttg.async_copy_global_to_local %130, %132 mask %134 other %cst_0 : tensor<64x64x!tt.ptr<bf16>, #blocked> -> <64x64xbf16, #shared, #smem, mutable, 2x64x64> loc(#loc24)
-      %136 = ttg.async_commit_group %135 loc(#loc24)
-      scf.yield %109, %112, %101, %arg14, %c2_i32, %arg16, %125, %arg18, %136 : tensor<128x64xf32, #mma>, i32, i32, i32, i32, !ttg.async.token, !ttg.async.token, !ttg.async.token, !ttg.async.token loc(#loc25)
+      %106 = ttg.local_load %105 token %102 : !ttg.memdesc<64x64xbf16, #shared, #smem, mutable, 2x64x64> -> tensor<64x64xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 2}>> loc(#loc24)
+      %107 = tt.dot %104, %106, %arg10, inputPrecision = tf32 : tensor<128x64xbf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 2}>> * tensor<64x64xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 2}>> -> tensor<128x64xf32, #mma> loc(#loc32)
+      %108 = arith.addi %arg11, %c1_i32 : i32 loc(#loc25)
+      %109 = arith.cmpi sge, %108, %c2_i32 : i32 loc(#loc25)
+      %110 = arith.select %109, %c0_i32, %108 : i32 loc(#loc25)
+      %111 = arith.addi %arg9, %c2_i32 : i32 loc(#loc25)
+      %112 = arith.muli %111, %c64_i32 : i32 loc(#loc33)
+      %113 = arith.subi %arg5, %112 : i32 loc(#loc28)
+      %114 = tt.splat %113 : i32 -> tensor<1x64xi32, #blocked> loc(#loc26)
+      %115 = arith.cmpi slt, %22, %114 : tensor<1x64xi32, #blocked> loc(#loc26)
+      %116 = tt.splat %112 : i32 -> tensor<128x64xi32, #blocked> loc(#loc29)
+      %117 = tt.addptr %25, %116 : tensor<128x64x!tt.ptr<bf16>, #blocked>, tensor<128x64xi32, #blocked> loc(#loc29)
+      %118 = tt.broadcast %115 : tensor<1x64xi1, #blocked> -> tensor<128x64xi1, #blocked> loc(#loc23)
+      %119 = ttg.memdesc_subview %39[%110, %c0_i32, %c0_i32] : !ttg.memdesc<2x128x64xbf16, #shared, #smem, mutable> -> !ttg.memdesc<128x64xbf16, #shared, #smem, mutable, 2x128x64> loc(#loc23)
+      %120 = tt.splat %98 : i1 -> tensor<128x64xi1, #blocked> loc(#loc25)
+      %121 = arith.andi %120, %118 : tensor<128x64xi1, #blocked> loc(#loc25)
+      %122 = ttg.async_copy_global_to_local %117, %119 mask %121 other %cst_1 : tensor<128x64x!tt.ptr<bf16>, #blocked> -> <128x64xbf16, #shared, #smem, mutable, 2x128x64> loc(#loc23)
+      %123 = ttg.async_commit_group %122 loc(#loc23)
+      %124 = tt.splat %113 : i32 -> tensor<64x1xi32, #blocked> loc(#loc27)
+      %125 = arith.cmpi slt, %27, %124 : tensor<64x1xi32, #blocked> loc(#loc27)
+      %126 = arith.muli %112, %arg7 : i32 loc(#loc30)
+      %127 = tt.splat %126 : i32 -> tensor<64x64xi32, #blocked> loc(#loc31)
+      %128 = tt.addptr %36, %127 : tensor<64x64x!tt.ptr<bf16>, #blocked>, tensor<64x64xi32, #blocked> loc(#loc31)
+      %129 = tt.broadcast %125 : tensor<64x1xi1, #blocked> -> tensor<64x64xi1, #blocked> loc(#loc24)
+      %130 = ttg.memdesc_subview %40[%110, %c0_i32, %c0_i32] : !ttg.memdesc<2x64x64xbf16, #shared, #smem, mutable> -> !ttg.memdesc<64x64xbf16, #shared, #smem, mutable, 2x64x64> loc(#loc24)
+      %131 = tt.splat %98 : i1 -> tensor<64x64xi1, #blocked> loc(#loc25)
+      %132 = arith.andi %131, %129 : tensor<64x64xi1, #blocked> loc(#loc25)
+      %133 = ttg.async_copy_global_to_local %128, %130 mask %132 other %cst_0 : tensor<64x64x!tt.ptr<bf16>, #blocked> -> <64x64xbf16, #shared, #smem, mutable, 2x64x64> loc(#loc24)
+      %134 = ttg.async_commit_group %133 loc(#loc24)
+      scf.yield %107, %110, %101, %arg14, %123, %arg16, %134 : tensor<128x64xf32, #mma>, i32, i32, !ttg.async.token, !ttg.async.token, !ttg.async.token, !ttg.async.token loc(#loc25)
     } loc(#loc25)
     %81 = ttg.async_wait  {num = 0 : i32} loc(#loc25)
     ttg.local_dealloc %40 : !ttg.memdesc<2x64x64xbf16, #shared, #smem, mutable> loc(#loc25)
