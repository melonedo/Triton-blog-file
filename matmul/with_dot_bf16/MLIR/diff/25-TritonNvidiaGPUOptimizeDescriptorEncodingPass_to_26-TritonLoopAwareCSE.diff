--- matmul/with_dot_bf16/MLIR/25-TritonNvidiaGPUOptimizeDescriptorEncodingPass.mlir
+++ matmul/with_dot_bf16/MLIR/26-TritonLoopAwareCSE.mlir
@@ -35,63 +35,60 @@
     %19 = arith.muli %16, %18 : tensor<128x1xi32, #blocked> loc(#loc11)
     %20 = tt.splat %arg0 : !tt.ptr<bf16> -> tensor<128x1x!tt.ptr<bf16>, #blocked> loc(#loc12)
     %21 = tt.addptr %20, %19 : tensor<128x1x!tt.ptr<bf16>, #blocked>, tensor<128x1xi32, #blocked> loc(#loc12)
-    %22 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #blocked}>> loc(#loc13)
-    %23 = tt.expand_dims %22 {axis = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #blocked}>> -> tensor<1x64xi32, #blocked> loc(#loc13)
-    %24 = tt.broadcast %21 : tensor<128x1x!tt.ptr<bf16>, #blocked> -> tensor<128x64x!tt.ptr<bf16>, #blocked> loc(#loc14)
-    %25 = tt.broadcast %23 : tensor<1x64xi32, #blocked> -> tensor<128x64xi32, #blocked> loc(#loc14)
-    %26 = tt.addptr %24, %25 : tensor<128x64x!tt.ptr<bf16>, #blocked>, tensor<128x64xi32, #blocked> loc(#loc14)
-    %27 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc15)
-    %28 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc15)
-    %29 = tt.expand_dims %27 {axis = 1 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<64x1xi32, #blocked> loc(#loc15)
-    %30 = tt.expand_dims %28 {axis = 1 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<64x1xi32, #blocked> loc(#loc15)
-    %31 = tt.splat %arg7 : i32 -> tensor<64x1xi32, #blocked> loc(#loc16)
-    %32 = arith.muli %29, %31 : tensor<64x1xi32, #blocked> loc(#loc16)
-    %33 = tt.splat %arg1 : !tt.ptr<bf16> -> tensor<64x1x!tt.ptr<bf16>, #blocked> loc(#loc17)
-    %34 = tt.addptr %33, %32 : tensor<64x1x!tt.ptr<bf16>, #blocked>, tensor<64x1xi32, #blocked> loc(#loc17)
-    %35 = tt.expand_dims %14 {axis = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #blocked}>> -> tensor<1x64xi32, #blocked> loc(#loc18)
-    %36 = tt.expand_dims %15 {axis = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #blocked1}>> -> tensor<1x64xi32, #blocked1> loc(#loc18)
-    %37 = tt.broadcast %34 : tensor<64x1x!tt.ptr<bf16>, #blocked> -> tensor<64x64x!tt.ptr<bf16>, #blocked> loc(#loc19)
-    %38 = tt.broadcast %35 : tensor<1x64xi32, #blocked> -> tensor<64x64xi32, #blocked> loc(#loc19)
-    %39 = tt.addptr %37, %38 : tensor<64x64x!tt.ptr<bf16>, #blocked>, tensor<64x64xi32, #blocked> loc(#loc19)
-    %40 = arith.addi %arg5, %c63_i32 : i32 loc(#loc43)
-    %41 = arith.divsi %40, %c64_i32 : i32 loc(#loc44)
-    %42 = scf.for %arg9 = %c0_i32 to %41 step %c1_i32 iter_args(%arg10 = %cst_1) -> (tensor<128x64xf32, #mma>)  : i32 {
-      %58 = arith.muli %arg9, %c64_i32 : i32 loc(#loc24)
-      %59 = arith.subi %arg5, %58 : i32 loc(#loc25)
-      %60 = tt.splat %59 : i32 -> tensor<1x64xi32, #blocked> loc(#loc26)
-      %61 = arith.cmpi slt, %23, %60 : tensor<1x64xi32, #blocked> loc(#loc26)
-      %62 = tt.splat %58 : i32 -> tensor<128x64xi32, #blocked> loc(#loc27)
-      %63 = tt.addptr %26, %62 : tensor<128x64x!tt.ptr<bf16>, #blocked>, tensor<128x64xi32, #blocked> loc(#loc27)
-      %64 = tt.broadcast %61 : tensor<1x64xi1, #blocked> -> tensor<128x64xi1, #blocked> loc(#loc28)
-      %65 = tt.load %63, %64, %cst_0 : tensor<128x64x!tt.ptr<bf16>, #blocked> loc(#loc28)
-      %66 = tt.splat %59 : i32 -> tensor<64x1xi32, #blocked> loc(#loc29)
-      %67 = arith.cmpi slt, %30, %66 : tensor<64x1xi32, #blocked> loc(#loc29)
-      %68 = arith.muli %58, %arg7 : i32 loc(#loc30)
-      %69 = tt.splat %68 : i32 -> tensor<64x64xi32, #blocked> loc(#loc31)
-      %70 = tt.addptr %39, %69 : tensor<64x64x!tt.ptr<bf16>, #blocked>, tensor<64x64xi32, #blocked> loc(#loc31)
-      %71 = tt.broadcast %67 : tensor<64x1xi1, #blocked> -> tensor<64x64xi1, #blocked> loc(#loc32)
-      %72 = tt.load %70, %71, %cst : tensor<64x64x!tt.ptr<bf16>, #blocked> loc(#loc32)
-      %73 = ttg.convert_layout %65 : tensor<128x64xbf16, #blocked> -> tensor<128x64xbf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 2}>> loc(#loc28)
-      %74 = ttg.convert_layout %72 : tensor<64x64xbf16, #blocked> -> tensor<64x64xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 2}>> loc(#loc32)
-      %75 = tt.dot %73, %74, %arg10, inputPrecision = tf32 : tensor<128x64xbf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 2}>> * tensor<64x64xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 2}>> -> tensor<128x64xf32, #mma> loc(#loc33)
-      scf.yield %75 : tensor<128x64xf32, #mma> loc(#loc34)
+    %22 = tt.expand_dims %10 {axis = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #blocked}>> -> tensor<1x64xi32, #blocked> loc(#loc13)
+    %23 = tt.broadcast %21 : tensor<128x1x!tt.ptr<bf16>, #blocked> -> tensor<128x64x!tt.ptr<bf16>, #blocked> loc(#loc14)
+    %24 = tt.broadcast %22 : tensor<1x64xi32, #blocked> -> tensor<128x64xi32, #blocked> loc(#loc14)
+    %25 = tt.addptr %23, %24 : tensor<128x64x!tt.ptr<bf16>, #blocked>, tensor<128x64xi32, #blocked> loc(#loc14)
+    %26 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc15)
+    %27 = tt.expand_dims %26 {axis = 1 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<64x1xi32, #blocked> loc(#loc15)
+    %28 = tt.splat %arg7 : i32 -> tensor<64x1xi32, #blocked> loc(#loc16)
+    %29 = arith.muli %27, %28 : tensor<64x1xi32, #blocked> loc(#loc16)
+    %30 = tt.splat %arg1 : !tt.ptr<bf16> -> tensor<64x1x!tt.ptr<bf16>, #blocked> loc(#loc17)
+    %31 = tt.addptr %30, %29 : tensor<64x1x!tt.ptr<bf16>, #blocked>, tensor<64x1xi32, #blocked> loc(#loc17)
+    %32 = tt.expand_dims %14 {axis = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #blocked}>> -> tensor<1x64xi32, #blocked> loc(#loc18)
+    %33 = tt.expand_dims %15 {axis = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #blocked1}>> -> tensor<1x64xi32, #blocked1> loc(#loc18)
+    %34 = tt.broadcast %31 : tensor<64x1x!tt.ptr<bf16>, #blocked> -> tensor<64x64x!tt.ptr<bf16>, #blocked> loc(#loc19)
+    %35 = tt.broadcast %32 : tensor<1x64xi32, #blocked> -> tensor<64x64xi32, #blocked> loc(#loc19)
+    %36 = tt.addptr %34, %35 : tensor<64x64x!tt.ptr<bf16>, #blocked>, tensor<64x64xi32, #blocked> loc(#loc19)
+    %37 = arith.addi %arg5, %c63_i32 : i32 loc(#loc43)
+    %38 = arith.divsi %37, %c64_i32 : i32 loc(#loc44)
+    %39 = scf.for %arg9 = %c0_i32 to %38 step %c1_i32 iter_args(%arg10 = %cst_1) -> (tensor<128x64xf32, #mma>)  : i32 {
+      %55 = arith.muli %arg9, %c64_i32 : i32 loc(#loc24)
+      %56 = arith.subi %arg5, %55 : i32 loc(#loc25)
+      %57 = tt.splat %56 : i32 -> tensor<1x64xi32, #blocked> loc(#loc26)
+      %58 = arith.cmpi slt, %22, %57 : tensor<1x64xi32, #blocked> loc(#loc26)
+      %59 = tt.splat %55 : i32 -> tensor<128x64xi32, #blocked> loc(#loc27)
+      %60 = tt.addptr %25, %59 : tensor<128x64x!tt.ptr<bf16>, #blocked>, tensor<128x64xi32, #blocked> loc(#loc27)
+      %61 = tt.broadcast %58 : tensor<1x64xi1, #blocked> -> tensor<128x64xi1, #blocked> loc(#loc28)
+      %62 = tt.load %60, %61, %cst_0 : tensor<128x64x!tt.ptr<bf16>, #blocked> loc(#loc28)
+      %63 = tt.splat %56 : i32 -> tensor<64x1xi32, #blocked> loc(#loc29)
+      %64 = arith.cmpi slt, %27, %63 : tensor<64x1xi32, #blocked> loc(#loc29)
+      %65 = arith.muli %55, %arg7 : i32 loc(#loc30)
+      %66 = tt.splat %65 : i32 -> tensor<64x64xi32, #blocked> loc(#loc31)
+      %67 = tt.addptr %36, %66 : tensor<64x64x!tt.ptr<bf16>, #blocked>, tensor<64x64xi32, #blocked> loc(#loc31)
+      %68 = tt.broadcast %64 : tensor<64x1xi1, #blocked> -> tensor<64x64xi1, #blocked> loc(#loc32)
+      %69 = tt.load %67, %68, %cst : tensor<64x64x!tt.ptr<bf16>, #blocked> loc(#loc32)
+      %70 = ttg.convert_layout %62 : tensor<128x64xbf16, #blocked> -> tensor<128x64xbf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 2}>> loc(#loc28)
+      %71 = ttg.convert_layout %69 : tensor<64x64xbf16, #blocked> -> tensor<64x64xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 2}>> loc(#loc32)
+      %72 = tt.dot %70, %71, %arg10, inputPrecision = tf32 : tensor<128x64xbf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 2}>> * tensor<64x64xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 2}>> -> tensor<128x64xf32, #mma> loc(#loc33)
+      scf.yield %72 : tensor<128x64xf32, #mma> loc(#loc34)
     } loc(#loc23)
-    %43 = tt.splat %arg8 : i32 -> tensor<128x1xi32, #blocked1> loc(#loc35)
-    %44 = arith.muli %17, %43 : tensor<128x1xi32, #blocked1> loc(#loc35)
-    %45 = tt.splat %arg2 : !tt.ptr<f32> -> tensor<128x1x!tt.ptr<f32>, #blocked1> loc(#loc36)
-    %46 = tt.addptr %45, %44 : tensor<128x1x!tt.ptr<f32>, #blocked1>, tensor<128x1xi32, #blocked1> loc(#loc36)
-    %47 = tt.broadcast %46 : tensor<128x1x!tt.ptr<f32>, #blocked1> -> tensor<128x64x!tt.ptr<f32>, #blocked1> loc(#loc37)
-    %48 = tt.broadcast %36 : tensor<1x64xi32, #blocked1> -> tensor<128x64xi32, #blocked1> loc(#loc37)
-    %49 = tt.addptr %47, %48 : tensor<128x64x!tt.ptr<f32>, #blocked1>, tensor<128x64xi32, #blocked1> loc(#loc37)
-    %50 = tt.splat %arg3 : i32 -> tensor<128x1xi32, #blocked1> loc(#loc38)
-    %51 = arith.cmpi slt, %17, %50 : tensor<128x1xi32, #blocked1> loc(#loc38)
-    %52 = tt.splat %arg4 : i32 -> tensor<1x64xi32, #blocked1> loc(#loc39)
-    %53 = arith.cmpi slt, %36, %52 : tensor<1x64xi32, #blocked1> loc(#loc39)
-    %54 = tt.broadcast %51 : tensor<128x1xi1, #blocked1> -> tensor<128x64xi1, #blocked1> loc(#loc40)
-    %55 = tt.broadcast %53 : tensor<1x64xi1, #blocked1> -> tensor<128x64xi1, #blocked1> loc(#loc40)
-    %56 = arith.andi %54, %55 : tensor<128x64xi1, #blocked1> loc(#loc40)
-    %57 = ttg.convert_layout %42 : tensor<128x64xf32, #mma> -> tensor<128x64xf32, #blocked1> loc(#loc41)
-    tt.store %49, %57, %56 : tensor<128x64x!tt.ptr<f32>, #blocked1> loc(#loc41)
+    %40 = tt.splat %arg8 : i32 -> tensor<128x1xi32, #blocked1> loc(#loc35)
+    %41 = arith.muli %17, %40 : tensor<128x1xi32, #blocked1> loc(#loc35)
+    %42 = tt.splat %arg2 : !tt.ptr<f32> -> tensor<128x1x!tt.ptr<f32>, #blocked1> loc(#loc36)
+    %43 = tt.addptr %42, %41 : tensor<128x1x!tt.ptr<f32>, #blocked1>, tensor<128x1xi32, #blocked1> loc(#loc36)
+    %44 = tt.broadcast %43 : tensor<128x1x!tt.ptr<f32>, #blocked1> -> tensor<128x64x!tt.ptr<f32>, #blocked1> loc(#loc37)
+    %45 = tt.broadcast %33 : tensor<1x64xi32, #blocked1> -> tensor<128x64xi32, #blocked1> loc(#loc37)
+    %46 = tt.addptr %44, %45 : tensor<128x64x!tt.ptr<f32>, #blocked1>, tensor<128x64xi32, #blocked1> loc(#loc37)
+    %47 = tt.splat %arg3 : i32 -> tensor<128x1xi32, #blocked1> loc(#loc38)
+    %48 = arith.cmpi slt, %17, %47 : tensor<128x1xi32, #blocked1> loc(#loc38)
+    %49 = tt.splat %arg4 : i32 -> tensor<1x64xi32, #blocked1> loc(#loc39)
+    %50 = arith.cmpi slt, %33, %49 : tensor<1x64xi32, #blocked1> loc(#loc39)
+    %51 = tt.broadcast %48 : tensor<128x1xi1, #blocked1> -> tensor<128x64xi1, #blocked1> loc(#loc40)
+    %52 = tt.broadcast %50 : tensor<1x64xi1, #blocked1> -> tensor<128x64xi1, #blocked1> loc(#loc40)
+    %53 = arith.andi %51, %52 : tensor<128x64xi1, #blocked1> loc(#loc40)
+    %54 = ttg.convert_layout %39 : tensor<128x64xf32, #mma> -> tensor<128x64xf32, #blocked1> loc(#loc41)
+    tt.store %46, %54, %53 : tensor<128x64x!tt.ptr<f32>, #blocked1> loc(#loc41)
     tt.return loc(#loc42)
   } loc(#loc)
 } loc(#loc)
