--- matmul/with_dot_bf16/MLIR/33-TritonGPUAssignLatencies.mlir
+++ matmul/with_dot_bf16/MLIR/34-TritonGPUScheduleLoops.mlir
@@ -53,26 +53,26 @@
     %37 = arith.addi %arg5, %c63_i32 : i32 loc(#loc43)
     %38 = arith.divsi %37, %c64_i32 : i32 loc(#loc44)
     %39 = scf.for %arg9 = %c0_i32 to %38 step %c1_i32 iter_args(%arg10 = %cst_1) -> (tensor<128x64xf32, #mma>)  : i32 {
-      %55 = arith.muli %arg9, %c64_i32 : i32 loc(#loc24)
-      %56 = arith.subi %arg5, %55 : i32 loc(#loc25)
-      %57 = tt.splat %56 : i32 -> tensor<1x64xi32, #blocked> loc(#loc26)
-      %58 = arith.cmpi slt, %22, %57 : tensor<1x64xi32, #blocked> loc(#loc26)
-      %59 = tt.splat %55 : i32 -> tensor<128x64xi32, #blocked> loc(#loc27)
-      %60 = tt.addptr %25, %59 : tensor<128x64x!tt.ptr<bf16>, #blocked>, tensor<128x64xi32, #blocked> loc(#loc27)
-      %61 = tt.broadcast %58 : tensor<1x64xi1, #blocked> -> tensor<128x64xi1, #blocked> loc(#loc28)
-      %62 = tt.load %60, %61, %cst_0 {tt.latency = 2 : i32} : tensor<128x64x!tt.ptr<bf16>, #blocked> loc(#loc28)
-      %63 = tt.splat %56 : i32 -> tensor<64x1xi32, #blocked> loc(#loc29)
-      %64 = arith.cmpi slt, %27, %63 : tensor<64x1xi32, #blocked> loc(#loc29)
-      %65 = arith.muli %55, %arg7 : i32 loc(#loc30)
-      %66 = tt.splat %65 : i32 -> tensor<64x64xi32, #blocked> loc(#loc31)
-      %67 = tt.addptr %36, %66 : tensor<64x64x!tt.ptr<bf16>, #blocked>, tensor<64x64xi32, #blocked> loc(#loc31)
-      %68 = tt.broadcast %64 : tensor<64x1xi1, #blocked> -> tensor<64x64xi1, #blocked> loc(#loc32)
-      %69 = tt.load %67, %68, %cst {tt.latency = 2 : i32} : tensor<64x64x!tt.ptr<bf16>, #blocked> loc(#loc32)
-      %70 = ttg.convert_layout %62 : tensor<128x64xbf16, #blocked> -> tensor<128x64xbf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 2}>> loc(#loc28)
-      %71 = ttg.convert_layout %69 : tensor<64x64xbf16, #blocked> -> tensor<64x64xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 2}>> loc(#loc32)
-      %72 = tt.dot %70, %71, %arg10, inputPrecision = tf32 : tensor<128x64xbf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 2}>> * tensor<64x64xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 2}>> -> tensor<128x64xf32, #mma> loc(#loc33)
+      %55 = arith.muli %arg9, %c64_i32 {loop.cluster = 2 : i32, loop.stage = 0 : i32} : i32 loc(#loc24)
+      %56 = arith.subi %arg5, %55 {loop.cluster = 2 : i32, loop.stage = 0 : i32} : i32 loc(#loc25)
+      %57 = tt.splat %56 {loop.cluster = 2 : i32, loop.stage = 0 : i32} : i32 -> tensor<1x64xi32, #blocked> loc(#loc26)
+      %58 = arith.cmpi slt, %22, %57 {loop.cluster = 2 : i32, loop.stage = 0 : i32} : tensor<1x64xi32, #blocked> loc(#loc26)
+      %59 = tt.splat %55 {loop.cluster = 2 : i32, loop.stage = 0 : i32} : i32 -> tensor<128x64xi32, #blocked> loc(#loc27)
+      %60 = tt.addptr %25, %59 {loop.cluster = 2 : i32, loop.stage = 0 : i32} : tensor<128x64x!tt.ptr<bf16>, #blocked>, tensor<128x64xi32, #blocked> loc(#loc27)
+      %61 = tt.broadcast %58 {loop.cluster = 2 : i32, loop.stage = 0 : i32} : tensor<1x64xi1, #blocked> -> tensor<128x64xi1, #blocked> loc(#loc28)
+      %62 = tt.load %60, %61, %cst_0 {loop.cluster = 2 : i32, loop.stage = 0 : i32} : tensor<128x64x!tt.ptr<bf16>, #blocked> loc(#loc28)
+      %63 = tt.splat %56 {loop.cluster = 2 : i32, loop.stage = 0 : i32} : i32 -> tensor<64x1xi32, #blocked> loc(#loc29)
+      %64 = arith.cmpi slt, %27, %63 {loop.cluster = 2 : i32, loop.stage = 0 : i32} : tensor<64x1xi32, #blocked> loc(#loc29)
+      %65 = arith.muli %55, %arg7 {loop.cluster = 2 : i32, loop.stage = 0 : i32} : i32 loc(#loc30)
+      %66 = tt.splat %65 {loop.cluster = 2 : i32, loop.stage = 0 : i32} : i32 -> tensor<64x64xi32, #blocked> loc(#loc31)
+      %67 = tt.addptr %36, %66 {loop.cluster = 2 : i32, loop.stage = 0 : i32} : tensor<64x64x!tt.ptr<bf16>, #blocked>, tensor<64x64xi32, #blocked> loc(#loc31)
+      %68 = tt.broadcast %64 {loop.cluster = 2 : i32, loop.stage = 0 : i32} : tensor<64x1xi1, #blocked> -> tensor<64x64xi1, #blocked> loc(#loc32)
+      %69 = tt.load %67, %68, %cst {loop.cluster = 2 : i32, loop.stage = 0 : i32} : tensor<64x64x!tt.ptr<bf16>, #blocked> loc(#loc32)
+      %70 = ttg.convert_layout %62 {loop.cluster = 0 : i32, loop.stage = 2 : i32} : tensor<128x64xbf16, #blocked> -> tensor<128x64xbf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 2}>> loc(#loc28)
+      %71 = ttg.convert_layout %69 {loop.cluster = 0 : i32, loop.stage = 2 : i32} : tensor<64x64xbf16, #blocked> -> tensor<64x64xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 2}>> loc(#loc32)
+      %72 = tt.dot %70, %71, %arg10, inputPrecision = tf32 {loop.cluster = 0 : i32, loop.stage = 2 : i32} : tensor<128x64xbf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 2}>> * tensor<64x64xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 2}>> -> tensor<128x64xf32, #mma> loc(#loc33)
       scf.yield %72 : tensor<128x64xf32, #mma> loc(#loc34)
-    } loc(#loc23)
+    } {tt.scheduled_max_stage = 2 : i32} loc(#loc23)
     %40 = tt.splat %arg8 : i32 -> tensor<128x1xi32, #blocked1> loc(#loc35)
     %41 = arith.muli %17, %40 : tensor<128x1xi32, #blocked1> loc(#loc35)
     %42 = tt.splat %arg2 : !tt.ptr<f32> -> tensor<128x1x!tt.ptr<f32>, #blocked1> loc(#loc36)
