--- matmul/with_dot_bf16/MLIR/48-SymbolDCE.mlir
+++ matmul/with_dot_bf16/MLIR/49-SCCP.mlir
@@ -7,17 +7,17 @@
 #smem = #ttg.shared_memory
 module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "cuda:86", "ttg.threads-per-warp" = 32 : i32} {
   tt.func public @matrix_multiplication_kernel(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32} loc("/workspace/Triton-blog-file/matmul/kernel/matmul-with-dot-bf16.py":6:0), %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32} loc("/workspace/Triton-blog-file/matmul/kernel/matmul-with-dot-bf16.py":6:0), %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32} loc("/workspace/Triton-blog-file/matmul/kernel/matmul-with-dot-bf16.py":6:0), %arg3: i32 {tt.divisibility = 16 : i32} loc("/workspace/Triton-blog-file/matmul/kernel/matmul-with-dot-bf16.py":6:0), %arg4: i32 {tt.divisibility = 16 : i32} loc("/workspace/Triton-blog-file/matmul/kernel/matmul-with-dot-bf16.py":6:0), %arg5: i32 {tt.divisibility = 16 : i32} loc("/workspace/Triton-blog-file/matmul/kernel/matmul-with-dot-bf16.py":6:0), %arg6: i32 {tt.divisibility = 16 : i32} loc("/workspace/Triton-blog-file/matmul/kernel/matmul-with-dot-bf16.py":6:0), %arg7: i32 {tt.divisibility = 16 : i32} loc("/workspace/Triton-blog-file/matmul/kernel/matmul-with-dot-bf16.py":6:0), %arg8: i32 {tt.divisibility = 16 : i32} loc("/workspace/Triton-blog-file/matmul/kernel/matmul-with-dot-bf16.py":6:0)) attributes {noinline = false} {
-    %cst = arith.constant dense<64> : tensor<128x64xi32, #blocked> loc(#loc1)
+    %cst = arith.constant dense<0.000000e+00> : tensor<128x64xf32, #mma> loc(#loc1)
+    %c0_i32 = arith.constant 0 : i32 loc(#loc1)
+    %c1_i32 = arith.constant 1 : i32 loc(#loc1)
+    %c63_i32 = arith.constant 63 : i32 loc(#loc1)
+    %c64_i32 = arith.constant 64 : i32 loc(#loc1)
+    %c128_i32 = arith.constant 128 : i32 loc(#loc1)
+    %cst_0 = arith.constant dense<0.000000e+00> : tensor<128x64xbf16, #blocked> loc(#loc1)
+    %cst_1 = arith.constant dense<0.000000e+00> : tensor<64x64xbf16, #blocked> loc(#loc1)
+    %c-1_i32 = arith.constant -1 : i32 loc(#loc1)
     %c2_i32 = arith.constant 2 : i32 loc(#loc1)
-    %c-1_i32 = arith.constant -1 : i32 loc(#loc1)
-    %cst_0 = arith.constant dense<0.000000e+00> : tensor<64x64xbf16, #blocked> loc(#loc1)
-    %cst_1 = arith.constant dense<0.000000e+00> : tensor<128x64xbf16, #blocked> loc(#loc1)
-    %c128_i32 = arith.constant 128 : i32 loc(#loc1)
-    %c64_i32 = arith.constant 64 : i32 loc(#loc1)
-    %c63_i32 = arith.constant 63 : i32 loc(#loc1)
-    %c1_i32 = arith.constant 1 : i32 loc(#loc1)
-    %c0_i32 = arith.constant 0 : i32 loc(#loc1)
-    %cst_2 = arith.constant dense<0.000000e+00> : tensor<128x64xf32, #mma> loc(#loc1)
+    %cst_2 = arith.constant dense<64> : tensor<128x64xi32, #blocked> loc(#loc1)
     %0 = tt.get_program_id x : i32 loc(#loc2)
     %1 = tt.get_program_id y : i32 loc(#loc3)
     %2 = arith.muli %1, %c128_i32 : i32 loc(#loc4)
@@ -66,7 +66,7 @@
     %45 = ttg.memdesc_subview %39[%c0_i32, %c0_i32, %c0_i32] : !ttg.memdesc<2x128x64xbf16, #shared, #smem, mutable> -> !ttg.memdesc<128x64xbf16, #shared, #smem, mutable, 2x128x64> loc(#loc23)
     %46 = tt.splat %41 : i1 -> tensor<128x64xi1, #blocked> loc(#loc25)
     %47 = arith.andi %46, %44 : tensor<128x64xi1, #blocked> loc(#loc25)
-    %48 = ttg.async_copy_global_to_local %25, %45 mask %47 other %cst_1 : tensor<128x64x!tt.ptr<bf16>, #blocked> -> <128x64xbf16, #shared, #smem, mutable, 2x128x64> loc(#loc23)
+    %48 = ttg.async_copy_global_to_local %25, %45 mask %47 other %cst_0 : tensor<128x64x!tt.ptr<bf16>, #blocked> -> <128x64xbf16, #shared, #smem, mutable, 2x128x64> loc(#loc23)
     %49 = ttg.async_commit_group %48 loc(#loc23)
     %50 = tt.splat %arg5 : i32 -> tensor<64x1xi32, #blocked> loc(#loc27)
     %51 = arith.cmpi slt, %27, %50 : tensor<64x1xi32, #blocked> loc(#loc27)
@@ -74,18 +74,18 @@
     %53 = ttg.memdesc_subview %40[%c0_i32, %c0_i32, %c0_i32] : !ttg.memdesc<2x64x64xbf16, #shared, #smem, mutable> -> !ttg.memdesc<64x64xbf16, #shared, #smem, mutable, 2x64x64> loc(#loc24)
     %54 = tt.splat %41 : i1 -> tensor<64x64xi1, #blocked> loc(#loc25)
     %55 = arith.andi %54, %52 : tensor<64x64xi1, #blocked> loc(#loc25)
-    %56 = ttg.async_copy_global_to_local %36, %53 mask %55 other %cst_0 : tensor<64x64x!tt.ptr<bf16>, #blocked> -> <64x64xbf16, #shared, #smem, mutable, 2x64x64> loc(#loc24)
+    %56 = ttg.async_copy_global_to_local %36, %53 mask %55 other %cst_1 : tensor<64x64x!tt.ptr<bf16>, #blocked> -> <64x64xbf16, #shared, #smem, mutable, 2x64x64> loc(#loc24)
     %57 = ttg.async_commit_group %56 loc(#loc24)
     %58 = arith.cmpi sgt, %38, %c1_i32 : i32 loc(#loc25)
     %59 = arith.subi %arg5, %c64_i32 : i32 loc(#loc28)
     %60 = tt.splat %59 : i32 -> tensor<1x64xi32, #blocked> loc(#loc26)
     %61 = arith.cmpi slt, %22, %60 : tensor<1x64xi32, #blocked> loc(#loc26)
-    %62 = tt.addptr %25, %cst : tensor<128x64x!tt.ptr<bf16>, #blocked>, tensor<128x64xi32, #blocked> loc(#loc29)
+    %62 = tt.addptr %25, %cst_2 : tensor<128x64x!tt.ptr<bf16>, #blocked>, tensor<128x64xi32, #blocked> loc(#loc29)
     %63 = tt.broadcast %61 : tensor<1x64xi1, #blocked> -> tensor<128x64xi1, #blocked> loc(#loc23)
     %64 = ttg.memdesc_subview %39[%c1_i32, %c0_i32, %c0_i32] : !ttg.memdesc<2x128x64xbf16, #shared, #smem, mutable> -> !ttg.memdesc<128x64xbf16, #shared, #smem, mutable, 2x128x64> loc(#loc23)
     %65 = tt.splat %58 : i1 -> tensor<128x64xi1, #blocked> loc(#loc25)
     %66 = arith.andi %65, %63 : tensor<128x64xi1, #blocked> loc(#loc25)
-    %67 = ttg.async_copy_global_to_local %62, %64 mask %66 other %cst_1 : tensor<128x64x!tt.ptr<bf16>, #blocked> -> <128x64xbf16, #shared, #smem, mutable, 2x128x64> loc(#loc23)
+    %67 = ttg.async_copy_global_to_local %62, %64 mask %66 other %cst_0 : tensor<128x64x!tt.ptr<bf16>, #blocked> -> <128x64xbf16, #shared, #smem, mutable, 2x128x64> loc(#loc23)
     %68 = ttg.async_commit_group %67 loc(#loc23)
     %69 = tt.splat %59 : i32 -> tensor<64x1xi32, #blocked> loc(#loc27)
     %70 = arith.cmpi slt, %27, %69 : tensor<64x1xi32, #blocked> loc(#loc27)
@@ -96,9 +96,9 @@
     %75 = ttg.memdesc_subview %40[%c1_i32, %c0_i32, %c0_i32] : !ttg.memdesc<2x64x64xbf16, #shared, #smem, mutable> -> !ttg.memdesc<64x64xbf16, #shared, #smem, mutable, 2x64x64> loc(#loc24)
     %76 = tt.splat %58 : i1 -> tensor<64x64xi1, #blocked> loc(#loc25)
     %77 = arith.andi %76, %74 : tensor<64x64xi1, #blocked> loc(#loc25)
-    %78 = ttg.async_copy_global_to_local %73, %75 mask %77 other %cst_0 : tensor<64x64x!tt.ptr<bf16>, #blocked> -> <64x64xbf16, #shared, #smem, mutable, 2x64x64> loc(#loc24)
+    %78 = ttg.async_copy_global_to_local %73, %75 mask %77 other %cst_1 : tensor<64x64x!tt.ptr<bf16>, #blocked> -> <64x64xbf16, #shared, #smem, mutable, 2x64x64> loc(#loc24)
     %79 = ttg.async_commit_group %78 loc(#loc24)
-    %80:7 = scf.for %arg9 = %c0_i32 to %38 step %c1_i32 iter_args(%arg10 = %cst_2, %arg11 = %c1_i32, %arg12 = %c-1_i32, %arg13 = %49, %arg14 = %68, %arg15 = %57, %arg16 = %79) -> (tensor<128x64xf32, #mma>, i32, i32, !ttg.async.token, !ttg.async.token, !ttg.async.token, !ttg.async.token)  : i32 {
+    %80:7 = scf.for %arg9 = %c0_i32 to %38 step %c1_i32 iter_args(%arg10 = %cst, %arg11 = %c1_i32, %arg12 = %c-1_i32, %arg13 = %49, %arg14 = %68, %arg15 = %57, %arg16 = %79) -> (tensor<128x64xf32, #mma>, i32, i32, !ttg.async.token, !ttg.async.token, !ttg.async.token, !ttg.async.token)  : i32 {
       %97 = arith.subi %38, %c2_i32 : i32 loc(#loc25)
       %98 = arith.cmpi slt, %arg9, %97 : i32 loc(#loc25)
       %99 = arith.addi %arg12, %c1_i32 : i32 loc(#loc25)
@@ -124,7 +124,7 @@
       %119 = ttg.memdesc_subview %39[%110, %c0_i32, %c0_i32] : !ttg.memdesc<2x128x64xbf16, #shared, #smem, mutable> -> !ttg.memdesc<128x64xbf16, #shared, #smem, mutable, 2x128x64> loc(#loc23)
       %120 = tt.splat %98 : i1 -> tensor<128x64xi1, #blocked> loc(#loc25)
       %121 = arith.andi %120, %118 : tensor<128x64xi1, #blocked> loc(#loc25)
-      %122 = ttg.async_copy_global_to_local %117, %119 mask %121 other %cst_1 : tensor<128x64x!tt.ptr<bf16>, #blocked> -> <128x64xbf16, #shared, #smem, mutable, 2x128x64> loc(#loc23)
+      %122 = ttg.async_copy_global_to_local %117, %119 mask %121 other %cst_0 : tensor<128x64x!tt.ptr<bf16>, #blocked> -> <128x64xbf16, #shared, #smem, mutable, 2x128x64> loc(#loc23)
       %123 = ttg.async_commit_group %122 loc(#loc23)
       %124 = tt.splat %113 : i32 -> tensor<64x1xi32, #blocked> loc(#loc27)
       %125 = arith.cmpi slt, %27, %124 : tensor<64x1xi32, #blocked> loc(#loc27)
@@ -135,7 +135,7 @@
       %130 = ttg.memdesc_subview %40[%110, %c0_i32, %c0_i32] : !ttg.memdesc<2x64x64xbf16, #shared, #smem, mutable> -> !ttg.memdesc<64x64xbf16, #shared, #smem, mutable, 2x64x64> loc(#loc24)
       %131 = tt.splat %98 : i1 -> tensor<64x64xi1, #blocked> loc(#loc25)
       %132 = arith.andi %131, %129 : tensor<64x64xi1, #blocked> loc(#loc25)
-      %133 = ttg.async_copy_global_to_local %128, %130 mask %132 other %cst_0 : tensor<64x64x!tt.ptr<bf16>, #blocked> -> <64x64xbf16, #shared, #smem, mutable, 2x64x64> loc(#loc24)
+      %133 = ttg.async_copy_global_to_local %128, %130 mask %132 other %cst_1 : tensor<64x64x!tt.ptr<bf16>, #blocked> -> <64x64xbf16, #shared, #smem, mutable, 2x64x64> loc(#loc24)
       %134 = ttg.async_commit_group %133 loc(#loc24)
       scf.yield %107, %110, %101, %arg14, %123, %arg16, %134 : tensor<128x64xf32, #mma>, i32, i32, !ttg.async.token, !ttg.async.token, !ttg.async.token, !ttg.async.token loc(#loc25)
     } loc(#loc25)
