--- matmul/with_dot_bf16/MLIR/54-SCFToControlFlowPass.mlir
+++ matmul/with_dot_bf16/MLIR/55-AllocateSharedMemory.mlir
@@ -9,7 +9,7 @@
 #mma = #ttg.nvidia_mma<{versionMajor = 2, versionMinor = 0, warpsPerCTA = [2, 2], instrShape = [16, 8]}>
 #shared = #ttg.swizzled_shared<{vec = 8, perPhase = 1, maxPhase = 8, order = [1, 0]}>
 #smem = #ttg.shared_memory
-module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "cuda:86", "ttg.threads-per-warp" = 32 : i32, "ttg.total-num-warps" = 4 : i32} {
+module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.shared = 49152 : i32, ttg.target = "cuda:86", "ttg.threads-per-warp" = 32 : i32, "ttg.total-num-warps" = 4 : i32} {
   tt.func public @matrix_multiplication_kernel(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32} loc("/workspace/Triton-blog-file/matmul/kernel/matmul-with-dot-bf16.py":6:0), %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32} loc("/workspace/Triton-blog-file/matmul/kernel/matmul-with-dot-bf16.py":6:0), %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32} loc("/workspace/Triton-blog-file/matmul/kernel/matmul-with-dot-bf16.py":6:0), %arg3: i32 {tt.divisibility = 16 : i32} loc("/workspace/Triton-blog-file/matmul/kernel/matmul-with-dot-bf16.py":6:0), %arg4: i32 {tt.divisibility = 16 : i32} loc("/workspace/Triton-blog-file/matmul/kernel/matmul-with-dot-bf16.py":6:0), %arg5: i32 {tt.divisibility = 16 : i32} loc("/workspace/Triton-blog-file/matmul/kernel/matmul-with-dot-bf16.py":6:0), %arg6: i32 {tt.divisibility = 16 : i32} loc("/workspace/Triton-blog-file/matmul/kernel/matmul-with-dot-bf16.py":6:0), %arg7: i32 {tt.divisibility = 16 : i32} loc("/workspace/Triton-blog-file/matmul/kernel/matmul-with-dot-bf16.py":6:0), %arg8: i32 {tt.divisibility = 16 : i32} loc("/workspace/Triton-blog-file/matmul/kernel/matmul-with-dot-bf16.py":6:0)) attributes {noinline = false} {
     %cst = arith.constant dense<0.000000e+00> : tensor<128x64xf32, #mma> loc(#loc1)
     %c0_i32 = arith.constant 0 : i32 loc(#loc1)
@@ -61,8 +61,8 @@
     %36 = tt.addptr %34, %35 : tensor<64x64x!tt.ptr<bf16>, #blocked>, tensor<64x64xi32, #blocked> loc(#loc19)
     %37 = arith.addi %arg5, %c63_i32 : i32 loc(#loc42)
     %38 = arith.divsi %37, %c64_i32 : i32 loc(#loc43)
-    %39 = ttg.local_alloc : () -> !ttg.memdesc<2x128x64xbf16, #shared, #smem, mutable> loc(#loc23)
-    %40 = ttg.local_alloc : () -> !ttg.memdesc<2x64x64xbf16, #shared, #smem, mutable> loc(#loc24)
+    %39 = ttg.local_alloc {allocation.offset = 0 : i32} : () -> !ttg.memdesc<2x128x64xbf16, #shared, #smem, mutable> loc(#loc23)
+    %40 = ttg.local_alloc {allocation.offset = 32768 : i32} : () -> !ttg.memdesc<2x64x64xbf16, #shared, #smem, mutable> loc(#loc24)
     %41 = arith.cmpi sgt, %38, %c0_i32 : i32 loc(#loc25)
     %42 = tt.splat %arg5 : i32 -> tensor<1x64xi32, #blocked> loc(#loc26)
     %43 = arith.cmpi slt, %22, %42 : tensor<1x64xi32, #blocked> loc(#loc26)
@@ -165,7 +165,7 @@
     %140 = tt.broadcast %137 : tensor<128x1xi1, #blocked1> -> tensor<128x64xi1, #blocked1> loc(#loc39)
     %141 = tt.broadcast %139 : tensor<1x64xi1, #blocked1> -> tensor<128x64xi1, #blocked1> loc(#loc39)
     %142 = arith.andi %140, %141 : tensor<128x64xi1, #blocked1> loc(#loc39)
-    %143 = ttg.convert_layout %81 : tensor<128x64xf32, #mma> -> tensor<128x64xf32, #blocked1> loc(#loc40)
+    %143 = ttg.convert_layout %81 {allocation.offset = 0 : i32} : tensor<128x64xf32, #mma> -> tensor<128x64xf32, #blocked1> loc(#loc40)
     tt.store %135, %143, %142 : tensor<128x64x!tt.ptr<f32>, #blocked1> loc(#loc40)
     tt.return loc(#loc41)
   } loc(#loc)
