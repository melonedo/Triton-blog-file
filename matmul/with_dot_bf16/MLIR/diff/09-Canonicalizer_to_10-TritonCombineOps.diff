--- matmul/with_dot_bf16/MLIR/09-Canonicalizer.mlir
+++ matmul/with_dot_bf16/MLIR/10-TritonCombineOps.mlir
@@ -39,8 +39,8 @@
     %26 = tt.broadcast %24 : tensor<64x1x!tt.ptr<bf16>> -> tensor<64x64x!tt.ptr<bf16>> loc(#loc20)
     %27 = tt.broadcast %25 : tensor<1x64xi32> -> tensor<64x64xi32> loc(#loc20)
     %28 = tt.addptr %26, %27 : tensor<64x64x!tt.ptr<bf16>>, tensor<64x64xi32> loc(#loc20)
-    %29 = arith.addi %arg5, %c63_i32 : i32 loc(#loc59)
-    %30 = arith.divsi %29, %c64_i32 : i32 loc(#loc60)
+    %29 = arith.addi %arg5, %c63_i32 : i32 loc(#loc58)
+    %30 = arith.divsi %29, %c64_i32 : i32 loc(#loc59)
     %31 = scf.for %arg9 = %c0_i32 to %30 step %c1_i32 iter_args(%arg10 = %cst_1) -> (tensor<128x64xf32>)  : i32 {
       %58 = arith.muli %arg9, %c64_i32 : i32 loc(#loc25)
       %59 = arith.subi %arg5, %58 : i32 loc(#loc26)
@@ -61,38 +61,37 @@
       %74 = tt.addptr %28, %73 : tensor<64x64x!tt.ptr<bf16>>, tensor<64x64xi32> loc(#loc36)
       %75 = tt.broadcast %70 : tensor<64x1xi1> -> tensor<64x64xi1> loc(#loc37)
       %76 = tt.load %74, %75, %cst : tensor<64x64x!tt.ptr<bf16>> loc(#loc37)
-      %77 = tt.dot %67, %76, %cst_1, inputPrecision = tf32 : tensor<128x64xbf16> * tensor<64x64xbf16> -> tensor<128x64xf32> loc(#loc38)
-      %78 = arith.addf %arg10, %77 : tensor<128x64xf32> loc(#loc39)
-      scf.yield %78 : tensor<128x64xf32> loc(#loc40)
+      %77 = tt.dot %67, %76, %arg10, inputPrecision = tf32 : tensor<128x64xbf16> * tensor<64x64xbf16> -> tensor<128x64xf32> loc(#loc38)
+      scf.yield %77 : tensor<128x64xf32> loc(#loc39)
     } loc(#loc24)
-    %32 = tt.expand_dims %5 {axis = 1 : i32} : tensor<128xi32> -> tensor<128x1xi32> loc(#loc41)
-    %33 = tt.splat %arg8 : i32 -> tensor<128x1xi32> loc(#loc42)
-    %34 = arith.muli %32, %33 : tensor<128x1xi32> loc(#loc42)
-    %35 = tt.splat %arg2 : !tt.ptr<f32> -> tensor<128x1x!tt.ptr<f32>> loc(#loc43)
-    %36 = tt.addptr %35, %34 : tensor<128x1x!tt.ptr<f32>>, tensor<128x1xi32> loc(#loc43)
-    %37 = tt.expand_dims %9 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32> loc(#loc44)
-    %38 = tt.broadcast %36 : tensor<128x1x!tt.ptr<f32>> -> tensor<128x64x!tt.ptr<f32>> loc(#loc45)
-    %39 = tt.broadcast %37 : tensor<1x64xi32> -> tensor<128x64xi32> loc(#loc45)
-    %40 = tt.addptr %38, %39 : tensor<128x64x!tt.ptr<f32>>, tensor<128x64xi32> loc(#loc45)
-    %41 = arith.muli %1, %c128_i32 : i32 loc(#loc46)
-    %42 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32> loc(#loc47)
-    %43 = tt.splat %41 : i32 -> tensor<128xi32> loc(#loc48)
-    %44 = arith.addi %43, %42 : tensor<128xi32> loc(#loc48)
-    %45 = arith.muli %0, %c64_i32 : i32 loc(#loc49)
-    %46 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32> loc(#loc50)
-    %47 = tt.splat %45 : i32 -> tensor<64xi32> loc(#loc51)
-    %48 = arith.addi %47, %46 : tensor<64xi32> loc(#loc51)
-    %49 = tt.expand_dims %44 {axis = 1 : i32} : tensor<128xi32> -> tensor<128x1xi32> loc(#loc52)
-    %50 = tt.splat %arg3 : i32 -> tensor<128x1xi32> loc(#loc53)
-    %51 = arith.cmpi slt, %49, %50 : tensor<128x1xi32> loc(#loc53)
-    %52 = tt.expand_dims %48 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32> loc(#loc54)
-    %53 = tt.splat %arg4 : i32 -> tensor<1x64xi32> loc(#loc55)
-    %54 = arith.cmpi slt, %52, %53 : tensor<1x64xi32> loc(#loc55)
-    %55 = tt.broadcast %51 : tensor<128x1xi1> -> tensor<128x64xi1> loc(#loc56)
-    %56 = tt.broadcast %54 : tensor<1x64xi1> -> tensor<128x64xi1> loc(#loc56)
-    %57 = arith.andi %55, %56 : tensor<128x64xi1> loc(#loc56)
-    tt.store %40, %31, %57 : tensor<128x64x!tt.ptr<f32>> loc(#loc57)
-    tt.return loc(#loc58)
+    %32 = tt.expand_dims %5 {axis = 1 : i32} : tensor<128xi32> -> tensor<128x1xi32> loc(#loc40)
+    %33 = tt.splat %arg8 : i32 -> tensor<128x1xi32> loc(#loc41)
+    %34 = arith.muli %32, %33 : tensor<128x1xi32> loc(#loc41)
+    %35 = tt.splat %arg2 : !tt.ptr<f32> -> tensor<128x1x!tt.ptr<f32>> loc(#loc42)
+    %36 = tt.addptr %35, %34 : tensor<128x1x!tt.ptr<f32>>, tensor<128x1xi32> loc(#loc42)
+    %37 = tt.expand_dims %9 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32> loc(#loc43)
+    %38 = tt.broadcast %36 : tensor<128x1x!tt.ptr<f32>> -> tensor<128x64x!tt.ptr<f32>> loc(#loc44)
+    %39 = tt.broadcast %37 : tensor<1x64xi32> -> tensor<128x64xi32> loc(#loc44)
+    %40 = tt.addptr %38, %39 : tensor<128x64x!tt.ptr<f32>>, tensor<128x64xi32> loc(#loc44)
+    %41 = arith.muli %1, %c128_i32 : i32 loc(#loc45)
+    %42 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32> loc(#loc46)
+    %43 = tt.splat %41 : i32 -> tensor<128xi32> loc(#loc47)
+    %44 = arith.addi %43, %42 : tensor<128xi32> loc(#loc47)
+    %45 = arith.muli %0, %c64_i32 : i32 loc(#loc48)
+    %46 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32> loc(#loc49)
+    %47 = tt.splat %45 : i32 -> tensor<64xi32> loc(#loc50)
+    %48 = arith.addi %47, %46 : tensor<64xi32> loc(#loc50)
+    %49 = tt.expand_dims %44 {axis = 1 : i32} : tensor<128xi32> -> tensor<128x1xi32> loc(#loc51)
+    %50 = tt.splat %arg3 : i32 -> tensor<128x1xi32> loc(#loc52)
+    %51 = arith.cmpi slt, %49, %50 : tensor<128x1xi32> loc(#loc52)
+    %52 = tt.expand_dims %48 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32> loc(#loc53)
+    %53 = tt.splat %arg4 : i32 -> tensor<1x64xi32> loc(#loc54)
+    %54 = arith.cmpi slt, %52, %53 : tensor<1x64xi32> loc(#loc54)
+    %55 = tt.broadcast %51 : tensor<128x1xi1> -> tensor<128x64xi1> loc(#loc55)
+    %56 = tt.broadcast %54 : tensor<1x64xi1> -> tensor<128x64xi1> loc(#loc55)
+    %57 = arith.andi %55, %56 : tensor<128x64xi1> loc(#loc55)
+    tt.store %40, %31, %57 : tensor<128x64x!tt.ptr<f32>> loc(#loc56)
+    tt.return loc(#loc57)
   } loc(#loc)
 } loc(#loc)
 #loc1 = loc(unknown)
@@ -133,25 +132,24 @@
 #loc36 = loc("/workspace/Triton-blog-file/matmul/kernel/matmul-with-dot-bf16.py":36:29)
 #loc37 = loc("/workspace/Triton-blog-file/matmul/kernel/matmul-with-dot-bf16.py":36:20)
 #loc38 = loc("/workspace/Triton-blog-file/matmul/kernel/matmul-with-dot-bf16.py":38:33)
-#loc39 = loc("/workspace/Triton-blog-file/matmul/kernel/matmul-with-dot-bf16.py":38:23)
-#loc40 = loc("/workspace/Triton-blog-file/matmul/kernel/matmul-with-dot-bf16.py":38:8)
-#loc41 = loc("/workspace/Triton-blog-file/matmul/kernel/matmul-with-dot-bf16.py":41:28)
-#loc42 = loc("/workspace/Triton-blog-file/matmul/kernel/matmul-with-dot-bf16.py":41:39)
-#loc43 = loc("/workspace/Triton-blog-file/matmul/kernel/matmul-with-dot-bf16.py":41:21)
-#loc44 = loc("/workspace/Triton-blog-file/matmul/kernel/matmul-with-dot-bf16.py":41:58)
-#loc45 = loc("/workspace/Triton-blog-file/matmul/kernel/matmul-with-dot-bf16.py":41:51)
-#loc46 = loc("/workspace/Triton-blog-file/matmul/kernel/matmul-with-dot-bf16.py":42:22)
-#loc47 = loc("/workspace/Triton-blog-file/matmul/kernel/matmul-with-dot-bf16.py":42:50)
-#loc48 = loc("/workspace/Triton-blog-file/matmul/kernel/matmul-with-dot-bf16.py":42:37)
-#loc49 = loc("/workspace/Triton-blog-file/matmul/kernel/matmul-with-dot-bf16.py":43:22)
-#loc50 = loc("/workspace/Triton-blog-file/matmul/kernel/matmul-with-dot-bf16.py":43:50)
-#loc51 = loc("/workspace/Triton-blog-file/matmul/kernel/matmul-with-dot-bf16.py":43:37)
-#loc52 = loc("/workspace/Triton-blog-file/matmul/kernel/matmul-with-dot-bf16.py":44:22)
-#loc53 = loc("/workspace/Triton-blog-file/matmul/kernel/matmul-with-dot-bf16.py":44:33)
-#loc54 = loc("/workspace/Triton-blog-file/matmul/kernel/matmul-with-dot-bf16.py":44:47)
-#loc55 = loc("/workspace/Triton-blog-file/matmul/kernel/matmul-with-dot-bf16.py":44:58)
-#loc56 = loc("/workspace/Triton-blog-file/matmul/kernel/matmul-with-dot-bf16.py":44:39)
-#loc57 = loc("/workspace/Triton-blog-file/matmul/kernel/matmul-with-dot-bf16.py":45:21)
-#loc58 = loc("/workspace/Triton-blog-file/matmul/kernel/matmul-with-dot-bf16.py":45:4)
-#loc59 = loc(callsite(#loc21 at #loc22))
-#loc60 = loc(callsite(#loc23 at #loc22))
+#loc39 = loc("/workspace/Triton-blog-file/matmul/kernel/matmul-with-dot-bf16.py":38:8)
+#loc40 = loc("/workspace/Triton-blog-file/matmul/kernel/matmul-with-dot-bf16.py":41:28)
+#loc41 = loc("/workspace/Triton-blog-file/matmul/kernel/matmul-with-dot-bf16.py":41:39)
+#loc42 = loc("/workspace/Triton-blog-file/matmul/kernel/matmul-with-dot-bf16.py":41:21)
+#loc43 = loc("/workspace/Triton-blog-file/matmul/kernel/matmul-with-dot-bf16.py":41:58)
+#loc44 = loc("/workspace/Triton-blog-file/matmul/kernel/matmul-with-dot-bf16.py":41:51)
+#loc45 = loc("/workspace/Triton-blog-file/matmul/kernel/matmul-with-dot-bf16.py":42:22)
+#loc46 = loc("/workspace/Triton-blog-file/matmul/kernel/matmul-with-dot-bf16.py":42:50)
+#loc47 = loc("/workspace/Triton-blog-file/matmul/kernel/matmul-with-dot-bf16.py":42:37)
+#loc48 = loc("/workspace/Triton-blog-file/matmul/kernel/matmul-with-dot-bf16.py":43:22)
+#loc49 = loc("/workspace/Triton-blog-file/matmul/kernel/matmul-with-dot-bf16.py":43:50)
+#loc50 = loc("/workspace/Triton-blog-file/matmul/kernel/matmul-with-dot-bf16.py":43:37)
+#loc51 = loc("/workspace/Triton-blog-file/matmul/kernel/matmul-with-dot-bf16.py":44:22)
+#loc52 = loc("/workspace/Triton-blog-file/matmul/kernel/matmul-with-dot-bf16.py":44:33)
+#loc53 = loc("/workspace/Triton-blog-file/matmul/kernel/matmul-with-dot-bf16.py":44:47)
+#loc54 = loc("/workspace/Triton-blog-file/matmul/kernel/matmul-with-dot-bf16.py":44:58)
+#loc55 = loc("/workspace/Triton-blog-file/matmul/kernel/matmul-with-dot-bf16.py":44:39)
+#loc56 = loc("/workspace/Triton-blog-file/matmul/kernel/matmul-with-dot-bf16.py":45:21)
+#loc57 = loc("/workspace/Triton-blog-file/matmul/kernel/matmul-with-dot-bf16.py":45:4)
+#loc58 = loc(callsite(#loc21 at #loc22))
+#loc59 = loc(callsite(#loc23 at #loc22))
